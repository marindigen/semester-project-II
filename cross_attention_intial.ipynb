{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import *\n",
    "from train import *\n",
    "\n",
    "# for tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with cross attention\n",
    "In this experiment, we sample data not only from uniform distribution but from variety of distributions. We then use this data for encoder and feed its output throught cross-attention mechanism to the decoder. The decoder takes the sequence with missing tokens and tries to fill in the gaps.\n",
    "\n",
    "### Architecture\n",
    "- We use multihead attention for the encoder and vanilla attention for the decoder.\n",
    "- In the encoder, before splitting heads I \"merge\" the 5 sequences through linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mariayuffa/semester-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train sequences of proteins sampled from Boltzmann distribution: (1000, 200)\n",
      "Loaded test sequences of proteins sampled from Boltzmann distribution: (1000, 200)\n",
      "Loaded train sequences of proteins for encoder distribution: (1000, 5, 200)\n",
      "Loaded test sequences of proteins for encoder distribution: (1000, 5, 200)\n"
     ]
    }
   ],
   "source": [
    "# Sequences for decoder\n",
    "final_chains_train = np.load('data/final_chains_T=1_num_iters=400_train.npy')\n",
    "print(\"Loaded train sequences of proteins sampled from Boltzmann distribution:\", final_chains_train.shape)\n",
    "\n",
    "final_chains_test = np.load('data/final_chains_T=1_num_iters=400_test.npy')\n",
    "print(\"Loaded test sequences of proteins sampled from Boltzmann distribution:\", final_chains_test.shape)\n",
    "\n",
    "\n",
    "# Sequences for encoder\n",
    "k = 0\n",
    "final_chains_encoder_train = np.zeros((1000,5,200))\n",
    "final_chains_encoder_train[:,0,:] = np.load('data/final_chains_T=1_num_iters=400_J=10_test.npy') \n",
    "final_chains_encoder_test = np.zeros((1000,5,200))\n",
    "final_chains_encoder_test[:,0,:] = np.load('data/final_chains_T=1_num_iters=400_J=10_test.npy') \n",
    "for i in range(20,100,20):\n",
    "    k+=1\n",
    "    final_chains_encoder_train[:,k,:] = np.load('data/final_chains_T=1_num_iters=400_J='+str(i)+'_train.npy')\n",
    "    final_chains_encoder_test[:,k,:] = np.load('data/final_chains_T=1_num_iters=400_J='+str(i)+'_test.npy') \n",
    "\n",
    "print(\"Loaded train sequences of proteins for encoder distribution:\", final_chains_encoder_train.shape)\n",
    "print(\"Loaded test sequences of proteins for encoder distribution:\", final_chains_encoder_test.shape)\n",
    "\n",
    "tensor_samples_train = torch.tensor(final_chains_encoder_train, dtype=torch.float32) \n",
    "tensor_samples_test = torch.tensor(final_chains_encoder_test, dtype=torch.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for different distributions\n",
    "distributions = [{\"type\": \"normal\", \"mean\": 0, \"std\": 1},\n",
    "        {\"type\": \"uniform\", \"low\": -1, \"high\": 1},\n",
    "        {\"type\": \"exponential\", \"scale\": 1},\n",
    "        {\"type\": \"gamma\", \"scale\": 1},\n",
    "        {\"type\": \"poisson\", \"lam\": 1.0}]\n",
    "\n",
    "# Example usage\n",
    "#tensor_samples_train = sample_data(1000, distributions, 200)  # 5 distributions, 1000 samples each, length of sequence 10\n",
    "#print(tensor_samples_train.shape)\n",
    "\n",
    "#tensor_samples_test = sample_data(1000, distributions, 200)  # 5 distributions, 1000 samples each, length of sequence 10\n",
    "#print(tensor_samples_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random data\n",
    "\n",
    "#data_train = torch.tensor([np.random.choice([-1, 1], size=20) for _ in range(1000)])\n",
    "#data_train_dec = torch.tensor([np.random.choice([-1, 1], size=20) for _ in range(1000)])\n",
    "#data_test = torch.tensor([np.random.choice([-1, 1], size=20) for _ in range(400)])\n",
    "#data_test_dec = torch.tensor([np.random.choice([-1, 1], size=20) for _ in range(400)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, a, max_seq_length, num_spins, proj_layer_dim, dropout, num_distr=5):\n",
    "        super(Transformer, self).__init__()\n",
    "        #self.encoder_embedding = nn.Embedding(num_spins, embed_dim)\n",
    "        #self.decoder_embedding = nn.Embedding(num_spins, embed_dim)\n",
    "        self.encoder_embeddings = nn.Linear(num_spins, embed_dim)\n",
    "        self.decoder_embeddings = nn.Linear(num_spins, embed_dim)\n",
    "\n",
    "        #self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        #self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.encoder_layer = EncoderLayer(embed_dim, proj_layer_dim, dropout, num_distr)\n",
    "        self.decoder_layer = DecoderLayer(embed_dim, a, max_seq_length, num_spins, proj_layer_dim, dropout)\n",
    "        self.fc = nn.Linear(embed_dim, num_spins)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        #src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        #src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        #tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "        src_embedded = self.encoder_embeddings(src)\n",
    "        tgt_embedded = self.decoder_embeddings(tgt)\n",
    "        enc_output = self.encoder_layer(src_embedded)\n",
    "        dec_output = self.decoder_layer(tgt_embedded, enc_output)\n",
    "        output = self.fc(dec_output)\n",
    "        #print(\"output of the transformer:\", output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters \n",
    "vocab_size = 3\n",
    "vocab = {-1:0,1:1,2:2} \n",
    "L = 200\n",
    "embedding_dim = 200\n",
    "proj_layer_dim = 128\n",
    "hidden_dim = 200\n",
    "num_layers = 1 # have to adapt the model for 2 and 3 layers\n",
    "dropout_rate = 0.0\n",
    "lr = 1e-3\n",
    "num_sequences = 1000\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]/Users/mariayuffa/semester-project/train.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_encoder_one_hot = torch.tensor(input_encoder_one_hot, dtype=torch.float)\n",
      "/Users/mariayuffa/semester-project/utils.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(one_hot, dtype=torch.float), torch.tensor(mask_positions, dtype=torch.long)\n",
      "100%|██████████| 1000/1000 [00:12<00:00, 82.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 104.7974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariayuffa/semester-project/train.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_encoder_one_hot = torch.tensor(input_encoder_one_hot, dtype=torch.float)\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 161.33it/s]\n",
      "  7%|▋         | 1/15 [00:18<04:17, 18.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Eval Loss: 0.5045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 92.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 102.3271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 587/1000 [00:03<00:02, 154.87it/s]\n",
      "                                              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#train(model, tensor_samples_train, final_chains_train, tensor_samples_test, final_chains_test, vocab, optimizer, criterion, one_hot_encoding=True, writer, device=device)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtraining_script\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_samples_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_train_dec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_chains_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_samples_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test_dec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_chains_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#torch.save(model.state_dict(), 'models/lstm_scratch.pt')\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#evaluate(model, test_dataloader, criterion, device=device)\u001b[39;00m\n",
      "File \u001b[0;32m~/semester-project/train.py:119\u001b[0m, in \u001b[0;36mtraining_script\u001b[0;34m(path, model, data_train, data_train_dec, data_test, data_test_dec, vocab, optimizer, criterion, one_hot_flag, num_epochs, device)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_script\u001b[39m(path, model, data_train, data_train_dec, data_test, data_test_dec, vocab, optimizer, criterion, one_hot_flag, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    118\u001b[0m     writer \u001b[38;5;241m=\u001b[39m SummaryWriter(path)\n\u001b[0;32m--> 119\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_train_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_flag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     writer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    121\u001b[0m     writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/semester-project/train.py:108\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_train, data_train_dec, data_test, data_test_dec, vocab, optimizer, criterion, one_hot_flag, writer, num_epochs, device)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(epoch_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    107\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data), epoch)\n\u001b[0;32m--> 108\u001b[0m eval_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_flag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEval Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_loss, epoch)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Eval Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(eval_loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/semester-project/train.py:29\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data_test, data_test_dec, vocab, criterion, one_hot_flag, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m input_seq_dec \u001b[38;5;241m=\u001b[39m data_test_dec[i]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m one_hot_flag:\n\u001b[0;32m---> 29\u001b[0m     input_encoder_one_hot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mone_hot_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq_enc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_seq_enc))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     30\u001b[0m     input_encoder_one_hot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_encoder_one_hot, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     32\u001b[0m     input_decoder_one_hot \u001b[38;5;241m=\u001b[39m one_hot_encoding(input_seq_dec\u001b[38;5;241m.\u001b[39mtolist(), vocab)\n",
      "File \u001b[0;32m~/semester-project/utils.py:53\u001b[0m, in \u001b[0;36mone_hot_encoding\u001b[0;34m(seq, vocab)\u001b[0m\n\u001b[1;32m     51\u001b[0m     one_hot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(seq), \u001b[38;5;28mlen\u001b[39m(vocab)), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, spin \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seq):\n\u001b[0;32m---> 53\u001b[0m         one_hot[i, vocab[spin]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m one_hot\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "model = Transformer(embed_dim=embedding_dim, a=0, max_seq_length=L, num_spins=3, proj_layer_dim=128, dropout=dropout_rate)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "path = 'runs/data_exp_distr_run_11_mask_pos=25'\n",
    "#train(model, tensor_samples_train, final_chains_train, tensor_samples_test, final_chains_test, vocab, optimizer, criterion, one_hot_encoding=True, writer, device=device)\n",
    "training_script(path, model, data_train=tensor_samples_train, data_train_dec=final_chains_train,\n",
    "                data_test=tensor_samples_test, data_test_dec=final_chains_test, vocab=vocab, optimizer=optimizer,\n",
    "                criterion=criterion, one_hot_flag=True, num_epochs=15, device=0)\n",
    "#torch.save(model.state_dict(), 'models/lstm_scratch.pt')\n",
    "#evaluate(model, test_dataloader, criterion, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To save only the decoder layer weights\n",
    "torch.save(model.decoder_layer.state_dict(), 'model_decoder/decoder_weights.pth')\n",
    "\n",
    "# If you need to load these weights later\n",
    "decoder_weights = torch.load('model_decoder/decoder_weights.pth')\n",
    "model.decoder_layer.load_state_dict(decoder_weights)\n",
    "\n",
    "# Save the weights of the FC layer\n",
    "torch.save(model.fc.state_dict(), 'model_decoder/transformer_fc_weights.pth')\n",
    "\n",
    "# To load these weights back into the FC layer later\n",
    "fc_weights = torch.load('model_decoder/transformer_fc_weights.pth')\n",
    "model.fc.load_state_dict(fc_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation study\n",
    "In this study we remove the encoder when testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAblated(nn.Module):\n",
    "    def __init__(self, embed_dim, a, max_seq_length, num_spins, proj_layer_dim, dropout):\n",
    "        super(TransformerAblated, self).__init__()\n",
    "        self.word_embeddings = nn.Linear(num_spins, embed_dim)\n",
    "        self.decoder_layer = DecoderLayer(embed_dim, a, max_seq_length, num_spins, proj_layer_dim, dropout)\n",
    "        self.fc = nn.Linear(embed_dim, num_spins)\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        tgt_embedded = self.word_embeddings(tgt)\n",
    "        dec_output = self.decoder_layer(tgt_embedded, tgt_embedded)\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n",
    "    \n",
    "# Create an instance of the new model\n",
    "new_model = TransformerAblated(embed_dim=embedding_dim, a=0, max_seq_length=L, num_spins=3, proj_layer_dim=128, dropout=dropout_rate)\n",
    "\n",
    "# Load the saved decoder weights\n",
    "decoder_weights = torch.load('model_decoder/decoder_weights.pth')\n",
    "new_model.decoder_layer.load_state_dict(decoder_weights)\n",
    "\n",
    "# Load the saved FC weights\n",
    "fc_weights = torch.load('model_decoder/transformer_fc_weights.pth')\n",
    "new_model.fc.load_state_dict(fc_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/transformer_ablation_run_3')\n",
    "\n",
    "def evaluate(new_model, data_test, data_test_dec, vocab, criterion, device=0):\n",
    "    new_model.eval()\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    for i, data in tqdm.tqdm(enumerate(data_test), total=len(data_test)):\n",
    "        # Get the inputs\n",
    "        input_seq_dec = data_test_dec[i]\n",
    "        input_decoder_one_hot = one_hot_encoding(input_seq_dec.tolist(), vocab)\n",
    "        # mask a token\n",
    "        masked_sequence_dec, positions = mask_random_spins(input_seq_dec, vocab, mask_token=2)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = new_model.forward(masked_sequence_dec)\n",
    "        predicted_tokens = F.softmax(outputs, dim=-1)\n",
    "        predictions = predicted_tokens[positions]\n",
    "        target_tokens = torch.where(input_decoder_one_hot[positions]==1)[1]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, target_tokens)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(data_test)\n",
    "\n",
    "def train(model, new_model, data_train, data_train_dec, data_test, data_test_dec, vocab, optimizer, criterion, num_epochs=20, device=0):\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    best_eval_loss = 1e-3 # used to do early stopping\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(num_epochs), leave=False, position=0):\n",
    "        running_loss = 0\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, data in tqdm.tqdm(enumerate(data_train), total=len(data_train)):\n",
    "            # Get the inputs\n",
    "            input_seq_enc = data\n",
    "            input_seq_dec = data_train_dec[i]\n",
    "\n",
    "            input_encoder_one_hot = torch.stack([one_hot_encoding(input_seq_enc[i].tolist(), vocab) for i in range(len(input_seq_enc))], dim=0)\n",
    "            input_encoder_one_hot = torch.tensor(input_encoder_one_hot, dtype=torch.float)\n",
    "\n",
    "            input_decoder_one_hot = one_hot_encoding(input_seq_dec.tolist(), vocab)\n",
    "\n",
    "            # mask a token in decoder\n",
    "            masked_sequence_dec, positions = mask_random_spins(input_seq_dec, vocab, mask_token=2)\n",
    "\n",
    "            # Forward pass\n",
    "            prediction = model.forward(input_encoder_one_hot, masked_sequence_dec) #masked_sequence[masked_position]\n",
    "            \n",
    "            predicted_tokens = F.softmax(prediction, dim=-1)\n",
    "            predictions = predicted_tokens[positions]\n",
    "            target_tokens = torch.where(input_decoder_one_hot[positions]==1)[1] #input_seq[masked_position]\n",
    "            \n",
    "            # Compute loss\n",
    "            #print(\"model prediction:\", prediction.shape)\n",
    "            #print(\"target:\", target_token.shape)\n",
    "            loss = criterion(predictions, target_tokens)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9. :    # print every 10 mini-batches\n",
    "                writer.add_scalar(\"Running Loss\", running_loss / 100, epoch)\n",
    "                #print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        new_model.decoder_layer = model.decoder_layer\n",
    "        new_model.fc = model.fc\n",
    "        print(f'Epoch {epoch + 1} | Train Loss: {(epoch_loss / len(data)):.4f}')\n",
    "        writer.add_scalar(\"Train Loss\", epoch_loss / len(data), epoch)\n",
    "        eval_loss = evaluate(new_model, data_test, data_test_dec, vocab, criterion, device=device)\n",
    "        writer.add_scalar(\"Eval Loss\", eval_loss, epoch)\n",
    "        print(f'Epoch {epoch + 1} | Eval Loss: {(eval_loss):.4f}')\n",
    "        \n",
    "        # Perform early stopping based on eval loss\n",
    "        if eval_loss < best_eval_loss:\n",
    "            return epoch_loss / len(data_train)\n",
    "    return epoch_loss / len(data_train)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 4\u001b[0m train(model, \u001b[43mnew_model\u001b[49m, tensor_samples_train, final_chains_train, tensor_samples_test, final_chains_test, vocab, optimizer, criterion, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define the parameters \u001b[39;00m\n\u001b[1;32m      7\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_model' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model, new_model, tensor_samples_train, final_chains_train, tensor_samples_test, final_chains_test, vocab, optimizer, criterion, device=device)\n",
    "\n",
    "# Define the parameters \n",
    "vocab_size = 3\n",
    "vocab = {-1:0,1:1,2:2} \n",
    "L = 200\n",
    "embedding_dim = 200\n",
    "proj_layer_dim = 128\n",
    "hidden_dim = 200\n",
    "num_layers = 1 # have to adapt the model for 2 and 3 layers\n",
    "dropout_rate = 0.0\n",
    "lr = 1e-3\n",
    "num_sequences = 1000\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_interactions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
