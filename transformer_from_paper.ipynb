{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A transformer-based model based on \"What does self-attention learn from Masked Language Modelling?\" paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core parts of the transformer\n",
    "- Separated position and spin\n",
    "- Single attention layer\n",
    "\n",
    "\n",
    "### Outline\n",
    "1. Vanilla attention implementation\n",
    "2. Factored attention implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100, 300])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(100,1000,300)\n",
    "idx = torch.randint(0,1000,(100,))\n",
    "# create masked_X, Y=model(masked_X)\n",
    "# X[:,idx,:]-Y[:,idx,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = torch.tensor([np.random.choice([-1, 1], size=20) for _ in range(1000)])\n",
    "data_test = torch.tensor([np.random.choice([-1, 1], size=20) for _ in range(100)])\n",
    "\n",
    "def mask_random_spin(sequence, mask_token=2):\n",
    "    \"\"\"\n",
    "    Mask one random spin in a sequence of protein spins.\n",
    "    \n",
    "    Parameters:\n",
    "    - sequence: a list or sequence of spins (integers)\n",
    "    - mask_token: the token used to mask a spin (default is 2)\n",
    "    \n",
    "    Returns:\n",
    "    - masked_sequence: a sequence similar to the input but with one spin masked\n",
    "    - masked_position: the position of the spin that was masked\n",
    "    \"\"\"\n",
    "    # Ensure the sequence can be converted to a list for masking\n",
    "    sequence_list = list(sequence)\n",
    "    \n",
    "    # Choose a random position to mask, excluding the first spin\n",
    "    mask_position = random.randint(1, len(sequence_list) - 1)\n",
    "    \n",
    "    # Mask the chosen position\n",
    "    masked_sequence = sequence_list.copy()\n",
    "    masked_sequence[mask_position] = mask_token\n",
    "    \n",
    "    return masked_sequence, mask_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaAttentionTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, a, max_seq_length, num_spins=2, dropout_rate=0.0):\n",
    "        super(VanillaAttentionTransformer, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(num_spins, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(max_seq_length, embed_dim)\n",
    "        self.a = a # parameter controlling how important are positions\n",
    "        self.value_weight = nn.Linear(embed_dim, embed_dim)\n",
    "        self.query_weight = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_weight = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_spins) # output layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, s, masked_token):\n",
    "        position_ids = torch.arange(len(s), dtype=torch.long)\n",
    "        position_ids = position_ids\n",
    "        x = self.word_embeddings(s) + self.position_embeddings(position_ids)\n",
    "        query = self.query_weight(x)\n",
    "        key = self.key_weight(x)\n",
    "\n",
    "        values = self.value_weight(self.word_embeddings(s) + self.a*self.position_embeddings(position_ids)) # (batch_size,embed_dim)\n",
    "        exp_scaling = torch.exp(self.word_embeddings(masked_token) + self.position_embeddings(position_ids).T@query.T@key@(self.word_embeddings(s) + self.a*self.position_embeddings(position_ids)))\n",
    "        attn_output = torch.sum(exp_scaling/torch.sum(exp_scaling.sum(0))*values) # not sure it multiplies the way I want it to multiply - check\n",
    "        output = self.dropout(self.fc(attn_output))\n",
    "        return output\n",
    "    \n",
    "class FactoredAttentionTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, a, max_seq_length, num_spins=2, dropout_rate=0.0):\n",
    "        super(FactoredAttentionTransformer, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(num_spins, embed_dim)\n",
    "        self.word_embedding.weight.requires_grad = False\n",
    "        self.position_embeddings = nn.Embedding(max_seq_length, embed_dim)\n",
    "        self.a = a # parameter controlling how important are positions\n",
    "        self.value_weight = nn.Linear(embed_dim, embed_dim)\n",
    "        self.query_weight = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_weight = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_spins) # output layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, s, masked_token):\n",
    "        # masked token should be equal to 0\n",
    "        # masked_token = torch.tensor([0])\n",
    "        position_ids = torch.arange(len(s), dtype=torch.long)\n",
    "        position_ids = position_ids\n",
    "        x = self.word_embeddings(s) + self.position_embeddings(position_ids)\n",
    "        query = self.query_weight(x)\n",
    "        key = self.key_weight(x)\n",
    "\n",
    "        values = self.value_weight(self.word_embeddings(s)) # (embed_dim)\n",
    "        exp_scaling = torch.exp(self.word_embeddings(masked_token) + self.position_embeddings(position_ids).T@query.T@key@(self.word_embeddings(s) + self.a*self.position_embeddings(position_ids)))\n",
    "        attn_output = torch.sum(exp_scaling/torch.sum(exp_scaling.sum(0))*values) # not sure it multiplies the way I want it to multiply - check\n",
    "        output = self.dropout(self.fc(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_test, criterion, device=0):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    for i, data in tqdm(enumerate(data_test), total=len(data_test)):\n",
    "        # Get the inputs\n",
    "        input_seq = data[i]\n",
    "        # mask a token\n",
    "        masked_sequence, masked_position = mask_random_spin(input_seq, mask_token=2)\n",
    "        # Forward pass\n",
    "        outputs = model.forward(masked_sequence, masked_sequence[masked_position])\n",
    "\n",
    "        output_token = F.log_softmax(outputs, dim=-1)\n",
    "        target_token = input_seq[masked_position]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output_token, target_token)\n",
    "        epoch_loss += loss.item()\n",
    "    #model.train()\n",
    "    return epoch_loss / len(data_test)\n",
    "\n",
    "def train(model, data_train, data_test, optimizer, criterion, num_epochs=8, device=0):\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    best_eval_loss = 1e-3 # used to do early stopping\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(num_epochs), leave=False, position=0):\n",
    "        running_loss = 0\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, data in tqdm.tqdm(enumerate(data_train), total=len(data_train)):\n",
    "            # Get the inputs\n",
    "            input_seq = data\n",
    "            print(input_seq)\n",
    "\n",
    "            # mask a token\n",
    "            masked_sequence, masked_position = mask_random_spin(input_seq, mask_token=2)\n",
    "            # Forward pass\n",
    "            prediction = model.forward(masked_sequence, masked_sequence[masked_position])\n",
    "            \n",
    "            predicted_token = F.log_softmax(prediction, dim=-1)\n",
    "            target_token = input_seq[masked_position]\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predicted_token, target_token)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9. :    # print every 10 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        print(f'Epoch {epoch + 1} | Train Loss: {(epoch_loss / len(data)):.4f}')\n",
    "        eval_loss = evaluate(model, data_test, criterion, device=device)\n",
    "        print(f'Epoch {epoch + 1} | Eval Loss: {(eval_loss):.4f}')\n",
    "        \n",
    "        # Perform early stopping based on eval loss\n",
    "        if eval_loss < best_eval_loss:\n",
    "            return epoch_loss / len(data_train)\n",
    "    return epoch_loss / len(data_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters \n",
    "vocab_size = 2\n",
    "L = 20\n",
    "embedding_dim = 20\n",
    "hidden_dim = 20\n",
    "num_layers = 1 # have to adapt the model for 2 and 3 layers\n",
    "dropout_rate = 0.0\n",
    "lr = 1e-3\n",
    "num_sequences = 1000\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1  1  1 -1 -1 -1 -1  1  1 -1 -1  1  1 -1  1 -1  1  1 -1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#torch.save(model.state_dict(), 'models/lstm_scratch.pt')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#evaluate(model, test_dataloader, criterion, device=device)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_train, data_test, optimizer, criterion, num_epochs, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m masked_sequence, masked_position \u001b[38;5;241m=\u001b[39m mask_random_spin(input_seq, mask_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmasked_position\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m predicted_token \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(prediction, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m target_token \u001b[38;5;241m=\u001b[39m input_seq[masked_position]\n",
      "Cell \u001b[0;32mIn[32], line 16\u001b[0m, in \u001b[0;36mVanillaAttentionTransformer.forward\u001b[0;34m(self, s, masked_token)\u001b[0m\n\u001b[1;32m     14\u001b[0m position_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(s), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     15\u001b[0m position_ids \u001b[38;5;241m=\u001b[39m position_ids\n\u001b[0;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[1;32m     17\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_weight(x)\n\u001b[1;32m     18\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_weight(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/protein_interactions/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/protein_interactions/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/protein_interactions/lib/python3.12/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/protein_interactions/lib/python3.12/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "model = VanillaAttentionTransformer(embed_dim=embedding_dim, a=1, max_seq_length=L, num_spins=2, dropout_rate=dropout_rate)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model, data_train, data_test, optimizer, criterion, device=device)\n",
    "#torch.save(model.state_dict(), 'models/lstm_scratch.pt')\n",
    "#evaluate(model, test_dataloader, criterion, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "- Do we pass through the sequence multiple times, each time masking different tokens or do we pass through the sequence once masking one random token?\n",
    "- Maybe we evaluate on how well the model predicts the other token in the sequence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_interactions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
